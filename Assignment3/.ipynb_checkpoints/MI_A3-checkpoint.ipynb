{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functions to preprocess the dataset\n",
    "'''\n",
    "\n",
    "# normalises the columns specified for that dataframe\n",
    "def normalize(df, columns):\n",
    "    result = df.copy()\n",
    "    for feature_name in columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "# clean the passed data frame\n",
    "def cleaning(df):\n",
    "    '''\n",
    "    df=df.dropna(thresh=7)\n",
    "    df[\"Age\"] = df.groupby(\"Community\").transform(lambda x: x.fillna(round(x.mean())))\n",
    "    df['Weight'] = df['Weight'].fillna(df.groupby('Age')['Weight'].transform('mean'))\n",
    "    #df['Delivery phase'] = df['Delivery phase'].fillna(round((df.groupby('Age')['Delivery phase'].transform('mean'))))\n",
    "    #df['Delivery phase'] -= 1\n",
    "    df['HB'] = df['HB'].fillna(round((df.groupby('Age')['HB'].transform('mean'))))\n",
    "    df['BP'] = df['BP'].fillna(df.groupby('Weight')['BP'].transform('mean'))\n",
    "    df['Delivery phase']=df['Delivery phase'].fillna(method='ffill')\n",
    "    df['Education'].fillna(5.0, inplace=True)\n",
    "    df['Residence']=df['Residence'].fillna(method='ffill')\n",
    "    df=df.fillna(method=\"ffill\")\n",
    "    normalized_df = normalize(df, ['Age','Weight','HB','BP',])\n",
    "    dum_df = pd.get_dummies(normalized_df, columns=[\"Community\"])\n",
    "    \n",
    "    return dum_df\n",
    "    '''\n",
    "    df=df.dropna(thresh=7)\n",
    "    df['Education'].fillna(5.0, inplace=True)\n",
    "    df['Residence']=df['Residence'].fillna(method='ffill')\n",
    "    df['Delivery phase']=df['Delivery phase'].fillna(method='ffill')\n",
    "    #df['Age']=df['Age'].fillna(df['Age'].mean())\n",
    "    df.loc[:,'Age'] = df.groupby(\"Community\").transform(lambda x: x.fillna(round(x.mean())))\n",
    "    df.loc[:,'Weight']=df['Weight'].fillna(df['Weight'].mean())\n",
    "    #df['Weight'] = df['Weight'].fillna(df.groupby('Age')['Weight'].transform('mean'))\n",
    "    df.loc[:,'HB']=df['HB'].fillna(df['HB'].mean())\n",
    "    #df['HB'] = df['HB'].fillna(round((df.groupby('Age')['HB'].transform('mean'))))\n",
    "    df.loc[:,'BP'] = df['BP'].fillna(df['BP'].mean())\n",
    "    #df['BP'] = df['BP'].fillna(df.groupby('Weight')['BP'].transform('mean'))\n",
    "    \n",
    "    df=df.fillna(method=\"ffill\")\n",
    "    normalized_df = normalize(df, ['Age','Weight','HB','BP',])\n",
    "    return normalized_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functions used to assist training the model\n",
    "'''\n",
    "# sigmoid activation function\n",
    "# Input :\n",
    "#       x -> value to apply sigmoid function on\n",
    "#       derivative -> calculate the derivative sigmoid function on x\n",
    "# Ouput:\n",
    "#       sigmoid(x) or derivative of sigmoid(x)\n",
    "def sigmoid(x, derivative = False):\n",
    "    if derivative:\n",
    "        return np.multiply(sigmoid(x),(1-sigmoid(x)))\n",
    "    return(1/(1 + np.exp(-x))) \n",
    "\n",
    "# ReLu activation function\n",
    "# Though not utilised here, it was tested as a possible candidate  \n",
    "# Input :\n",
    "#       x -> value to apply ReLU function on\n",
    "#       derivative -> calculate the derivative ReLU function on x\n",
    "# Ouput:\n",
    "#       ReLU(x) or derivative of ReLU(x)\n",
    "def ReLU(x, derivative = False):\n",
    "    if derivative:\n",
    "        return 1 if x.flatten()[0] > 0 else 0\n",
    "    return x * (x.flatten()[0] > 0)\n",
    "\n",
    "# Swish activation function\n",
    "# Though not utilised here, it was tested as a possible candidate \n",
    "# Input :\n",
    "#       x -> value to apply swish function on\n",
    "#       derivative -> calculate the derivative swish function on x\n",
    "# Ouput:\n",
    "#       swish(x) or derivative of swish(x)\n",
    "def swish(x, derivative = False):\n",
    "    if derivative:\n",
    "        return x / (1. + np.exp(-x)) + (1. / (1. + np.exp(-x))) * (1. - x * (1. / (1. + np.exp(-x))))\n",
    "    return x*sigmoid(x)\n",
    "\n",
    "# tanh activation function\n",
    "# Input :\n",
    "#       x -> value to apply hyperbolic tangent function on\n",
    "#       derivative -> calculate the derivative hyperbolic tangent function on x\n",
    "# Ouput:\n",
    "#       tanh_f(x) or derivative of tanh_f(x)\n",
    "def tanh_f(x, derivative = False):\n",
    "    if derivative:\n",
    "        return 1 - np.power(x,2)\n",
    "    return np.tanh(x)\n",
    "\n",
    "# stores weights and other hyper parameters of the hidden layers in a dictionary parameters\n",
    "# n_x -> Number of columns in training samples \n",
    "# n_hidden1 -> Number of hidden neurons in hidden layer 1 -> 512\n",
    "# n_hidden2 -> Number of hidden neurons in hidden layer 2 -> 256\n",
    "# n_hidden3 -> Number of hidden neurons in hidden layer 3 -> 256\n",
    "# n_y -> Number of output classes -> 1\n",
    "# alpha -> learning rate -> 8e-6\n",
    "# max_epochs -> max number of epochs the model will run -> 100\n",
    "# momentum -> momentum used to implement nesterovs momentum\n",
    "# decay_rate -> Rate at which the learning rate is \n",
    "#               decreased by during each epoch -> alpha/epochs\n",
    "# n_train -> Number of training samples\n",
    "# n_test -> Number of testing samples\n",
    "def initialize_parameters(n_x, n_hidden1, n_hidden2, n_hidden3,\n",
    "                        n_y, alpha, max_epochs, momentum,\n",
    "                        decay_rate, n_train,\n",
    "                        n_test):\n",
    "    parameters = dict()\n",
    "    # Use Xaviers initialization of weights, good for tanh function\n",
    "    # (512, 9)\n",
    "    parameters['w1'] = np.random.randn(n_hidden1,n_x)*np.sqrt(2/(n_hidden1))\n",
    "    # (512,1)\n",
    "    parameters['b1'] = np.random.rand(n_hidden1,1)\n",
    "    # (256, 512)\n",
    "    parameters['w2'] = np.random.randn(n_hidden2,n_hidden1)*np.sqrt(2/(n_hidden2)) \n",
    "    # (256,1)\n",
    "    parameters['b2'] = np.random.rand(n_hidden2,1)\n",
    "    # (512, 256)\n",
    "    parameters['w3'] = np.random.randn(n_hidden3,n_hidden2)*np.sqrt(2/(n_hidden3))\n",
    "    # (512,1)\n",
    "    parameters['b3'] = np.random.rand(n_hidden3,1)\n",
    "    # (1,512)\n",
    "    parameters['w4'] = np.random.randn(n_y,n_hidden3)*np.sqrt(2/(n_y)) \n",
    "    # (1,1)\n",
    "    parameters['b4'] = np.random.rand(n_y,1)\n",
    "    parameters['alpha'] = alpha\n",
    "    parameters['initial_alpha'] = alpha\n",
    "    parameters['max_epochs'] = epochs\n",
    "    parameters['beta'] = momentum\n",
    "    \n",
    "    # failed implementation of dropout regularization\n",
    "    #parameters['do_dropout'] = do_dropout\n",
    "    #parameters['dropout_percent'] = dropout_percent\n",
    "    \n",
    "    parameters['decay'] = decay_rate\n",
    "    # velocity factors which help in calculating \n",
    "    # gradients using momentum\n",
    "    parameters['v_w1'] = 0\n",
    "    parameters['v_w2'] = 0\n",
    "    parameters['v_w3'] = 0\n",
    "    parameters['v_w4'] = 0\n",
    "    parameters['v_b1'] = 0\n",
    "    parameters['v_b2'] = 0\n",
    "    parameters['v_b3'] = 0\n",
    "    parameters['v_b4'] = 0\n",
    "    parameters['n_train'] = n_train\n",
    "    parameters['n_test'] = n_test\n",
    "    # failed implementation of batch gradient descent\n",
    "    #parameters['batch_size'] = batch_size\n",
    "    return parameters\n",
    "\n",
    "# binary cross entropy for 2 class classification\n",
    "def compute_loss(y_hats, Y, parameters):\n",
    "    \n",
    "    W1 = parameters['w1']\n",
    "    W2 = parameters['w2']\n",
    "    W3 = parameters['w3']\n",
    "    n_train = parameters['n_train']\n",
    "    lambd = 0.7\n",
    "    logloss = np.multiply(np.log(y_hats), Y) + np.multiply(np.log(1-y_hats), (1-Y))\n",
    "    bce = - np.sum(logloss)/len(Y)\n",
    "    \n",
    "    # failed implementation of L2 regularization\n",
    "    l2_reg = (np.sum(np.square(W1)))*(lambd/len(Y)) + \\\n",
    "        (np.sum(np.square(W2)))*(lambd/(2*len(Y))) + \\\n",
    "        (np.sum(np.square(W3)))*(lambd/(3*len(Y)))\n",
    "    return bce\n",
    "    \n",
    "# accuracy for binary classification\n",
    "# tp = true positive\n",
    "# tn = true negative\n",
    "# Accuracy = (tp+tn)/total samples\n",
    "def compute_acc(y_hats, Y):\n",
    "    tp,tn = 0,0\n",
    "    for i in range(len(Y)):\n",
    "        y_hats[i] = 1 if y_hats[i] > 0.6 else 0\n",
    "        if(Y[i]==1 and y_hats[i]==1):\n",
    "            tp=tp+1\n",
    "        if(Y[i]==0 and y_hats[i]==0):\n",
    "            tn=tn+1\n",
    "    return (tp+tn)/len(Y)\n",
    "        \n",
    "# Forward propagation\n",
    "# Predict the value y_train using x_train\n",
    "# Activation function stack\n",
    "# tanh\n",
    "# tanh\n",
    "# tanh\n",
    "# sigmoid\n",
    "def forward_propagation(x_train, parameters, do_dropout=False):\n",
    "    W1 = parameters['w1']\n",
    "    W2 = parameters['w2']\n",
    "    W3 = parameters['w3']\n",
    "    W4 = parameters['w4']\n",
    "    b1 = parameters['b1']\n",
    "    b2 = parameters['b2']\n",
    "    b3 = parameters['b3']\n",
    "    b4 = parameters['b4']\n",
    "    alpha = parameters['alpha']\n",
    "    n_train = parameters['n_train']\n",
    "    \n",
    "    #dropout_percent = parameters['dropout_percent']\n",
    "    #keep_probability = 1 - dropout_percent\n",
    "    \n",
    "    # store all intermediate values\n",
    "    cache = dict()\n",
    "    #forward propagation begins\n",
    "    \n",
    "    # action in hidden 1 layer\n",
    "    # (512,9) . (9,1) = (512,1)\n",
    "    z1 = np.dot(W1,x_train) + b1\n",
    "    a1 = tanh_f(z1)\n",
    "    \n",
    "    #if do_dropout:\n",
    "        #a1 *= dropout_mask1\n",
    "        \n",
    "    # action in hidden 2 layer\n",
    "    # (256, 512) . (512,1) = (256,1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = tanh_f(z2)\n",
    "    \n",
    "    #if do_dropout:\n",
    "    #a2 *= dropout_mask2\n",
    "    #a2 /= keep_probability\n",
    "    \n",
    "    # (512,256) . (256,1) = (512,1)\n",
    "    z3 = np.dot(W3, a2) + b3\n",
    "    a3 = tanh_f(z3)\n",
    "    \n",
    "    # (1,512) . (512,1) = (1,1)\n",
    "    z4 = np.dot(W4,a3) + b4\n",
    "    a4 = sigmoid(z4)\n",
    "    \n",
    "    # copy values to cache\n",
    "    #cache['dropout_mask1'] = dropout_mask1\n",
    "    #cache['dropout_mask2'] = dropout_mask2\n",
    "    cache['z1'] = z1\n",
    "    cache['a1'] = a1\n",
    "    cache['z2'] = z2\n",
    "    cache['a2'] = a2\n",
    "    cache['z3'] = z3\n",
    "    cache['a3'] = a3\n",
    "    cache['a4'] = a4\n",
    "    \n",
    "    # returns y_hat and intermediate cache values\n",
    "    return a4,cache\n",
    "\n",
    "# Used to calculate the gradients for \n",
    "# back propagation. \n",
    "# Stores all gradients in intermediate \n",
    "# dictionary grads_cache\n",
    "def calculate_gradients(x_train, y_train, parameters,cache, do_dropout=False):\n",
    "    # load the parameters to be updated\n",
    "    W1 = parameters['w1']\n",
    "    W2 = parameters['w2']\n",
    "    W3 = parameters['w3']\n",
    "    W4 = parameters['w4']\n",
    "    b1 = parameters['b1']\n",
    "    b2 = parameters['b2']\n",
    "    b3 = parameters['b3']\n",
    "    b4 = parameters['b4']\n",
    "    n_train = parameters['n_train']\n",
    "    v_w1 = parameters['v_w1']\n",
    "    v_w2 = parameters['v_w2']\n",
    "    v_w3 = parameters['v_w3']\n",
    "    v_w4 = parameters['v_w4']\n",
    "    v_b1 = parameters['v_b1']\n",
    "    v_b2 = parameters['v_b2']\n",
    "    v_b3 = parameters['v_b3']\n",
    "    v_b4 = parameters['v_b4']\n",
    "    \n",
    "    # unload values from cache\n",
    "    z1 = cache['z1'] \n",
    "    a1 = cache['a1']\n",
    "    z2 = cache['z2'] \n",
    "    a2 = cache['a2'] \n",
    "    z3 = cache['z3'] \n",
    "    a3 = cache['a3'] \n",
    "    a4 = cache['a4']\n",
    "    \n",
    "    grads_cache = dict()\n",
    "    # (1,1) - (1,1) = (1,1)\n",
    "    d_z4 =  a4 - y_train\n",
    "    \n",
    "    # (1,1) .(1,512) = (1,512)\n",
    "    d_W4 = np.dot(d_z4, a3.transpose()) \n",
    "    d_b4 = np.sum(d_z4, axis = 1, keepdims=True)\n",
    "    \n",
    "    # (512,1) . (1,1) = (512,1)\n",
    "    # (512,1) * (512,1) = (512,1)\n",
    "    d_z3 = np.multiply(np.dot(W4.transpose(), d_z4), tanh_f(a3, derivative=True))\n",
    "    # (512,1) . (1, 256) = (512,256)\n",
    "    d_W3 = np.dot(d_z3, a2.transpose()) \n",
    "    d_b3 = np.sum(d_z3, axis=1, keepdims=True)\n",
    "    \n",
    "    # (256,512) . (512,1) = (256,1)\n",
    "    # (256, 1) * (256,1) = (256,1)\n",
    "    d_z2 = np.multiply(np.dot(W3.transpose(), d_z3), tanh_f(a2, derivative=True))\n",
    "    # (256,1) . (1, 512) = (256, 512)\n",
    "    d_W2 =  np.dot(d_z2, a1.transpose()) \n",
    "    d_b2 = np.sum(d_z2, axis=1, keepdims=True)\n",
    "    \n",
    "    # (512,256) . (256,1) = (512,1)\n",
    "    # (512,1) * (512,1) = (512,1)\n",
    "    d_z1 = np.multiply(np.dot(W2.transpose(), d_z2), tanh_f(a1, derivative=True))\n",
    "    # (512,1) . (1, 9) = (512,9)\n",
    "    d_W1 = np.dot(d_z1, x_train.transpose())\n",
    "    d_b1 = np.sum(d_z1, axis=1, keepdims=True)\n",
    "    \n",
    "    # save gradients to grad_cache\n",
    "    grads_cache['d_W4'] = d_W4\n",
    "    grads_cache['d_b4'] = d_b4\n",
    "    grads_cache['d_W3'] = d_W3\n",
    "    grads_cache['d_b3'] = d_b3\n",
    "    grads_cache['d_W2'] = d_W2\n",
    "    grads_cache['d_b2'] = d_b2\n",
    "    grads_cache['d_W1'] = d_W1\n",
    "    grads_cache['d_b1'] = d_b1\n",
    "    return grads_cache\n",
    "\n",
    "# Perform the second part of back propagation\n",
    "# update parameters for that epoch\n",
    "def update_parameters(parameters, grad_cache):\n",
    "    # load parameters\n",
    "    W1 = parameters['w1']\n",
    "    W2 = parameters['w2']\n",
    "    W3 = parameters['w3']\n",
    "    W4 = parameters['w4']\n",
    "    b1 = parameters['b1']\n",
    "    b2 = parameters['b2']\n",
    "    b3 = parameters['b3']\n",
    "    b4 = parameters['b4']\n",
    "    alpha = parameters['alpha']\n",
    "    n_train = parameters['n_train']\n",
    "    momentum = parameters['beta']\n",
    "\n",
    "    v_w1 = parameters['v_w1']\n",
    "    v_w2 = parameters['v_w2']\n",
    "    v_w3 = parameters['v_w3']\n",
    "    v_w4 = parameters['v_w4']\n",
    "    v_b1 = parameters['v_b1']\n",
    "    v_b2 = parameters['v_b2']\n",
    "    v_b3 = parameters['v_b3']\n",
    "    v_b4 = parameters['v_b4']\n",
    "    \n",
    "    # load gradient cache\n",
    "    d_W1 = grad_cache['d_W1']\n",
    "    d_W2 = grad_cache['d_W2']\n",
    "    d_W3 = grad_cache['d_W3']\n",
    "    d_W4 = grad_cache['d_W4']\n",
    "    d_b1 = grad_cache['d_b1']\n",
    "    d_b2 = grad_cache['d_b2']\n",
    "    d_b3 = grad_cache['d_b3']\n",
    "    d_b4 = grad_cache['d_b4']\n",
    "    \n",
    "    # nesterov momentum\n",
    "    # implementation taken from\n",
    "    # https://arxiv.org/pdf/1607.01981.pdf\n",
    "    v_w1_old = v_w1\n",
    "    v_w1 = momentum*v_w1 - alpha*d_W1\n",
    "    v_w2_old = v_w2\n",
    "    v_w2 = momentum*v_w2 - alpha*d_W2\n",
    "    v_w3_old = v_w3\n",
    "    v_w3 = momentum*v_w3 - alpha*d_W3\n",
    "    v_w4_old = v_w4\n",
    "    v_w4 = momentum*v_w4 - alpha*d_W4\n",
    "    v_b1_old = v_b1\n",
    "    v_b1 = momentum*v_b1 - alpha*d_b1\n",
    "    v_b2_old = v_b2\n",
    "    v_b2 = momentum*v_b2 - alpha*d_b2\n",
    "    v_b3_old = v_b3\n",
    "    v_b3 = momentum*v_b3 - alpha*d_b3\n",
    "    v_b4_old = v_b4\n",
    "    v_b4 = momentum*v_b4 - alpha*d_b4\n",
    "    #update parameters\n",
    "    parameters['v_w1'] = v_w1\n",
    "    parameters['v_w2'] = v_w2\n",
    "    parameters['v_w3'] = v_w3\n",
    "    parameters['v_w4'] = v_w4\n",
    "    parameters['v_b1'] = v_b1\n",
    "    parameters['v_b2'] = v_b2\n",
    "    parameters['v_b3'] = v_b3\n",
    "    parameters['v_b4'] = v_b4\n",
    "    \n",
    "    parameters['w1'] = W1 - momentum*v_w1_old + v_w1*(1+momentum)\n",
    "    parameters['b1'] = b1 - momentum*v_b1_old + v_b1*(1+momentum)\n",
    "    parameters['w2'] = W2 - momentum*v_w2_old + v_w2*(1+momentum)\n",
    "    parameters['b2'] = b2 - momentum*v_b2_old + v_b2*(1+momentum)\n",
    "    parameters['w3'] = W3 - momentum*v_w3_old + v_w3*(1+momentum)\n",
    "    parameters['b3'] = b3 - momentum*v_b3_old + v_b3*(1+momentum)\n",
    "    parameters['w4'] = W4 - momentum*v_w4_old + v_w4*(1+momentum)\n",
    "    parameters['b4'] = b4 - momentum*v_b4_old + v_b4*(1+momentum)\n",
    "    '''\n",
    "    Regular SGD\n",
    "    parameters['w1'] = W1 - alpha*d_W1\n",
    "    parameters['b1'] = b1 - alpha*d_b1\n",
    "    parameters['w2'] = W2 - alpha*d_W2\n",
    "    parameters['b2'] = b2 - alpha*d_b2\n",
    "    parameters['w3'] = W3 - alpha*d_W3\n",
    "    parameters['b3'] = b3 - alpha*d_b3\n",
    "    parameters['w4'] = W4 - alpha*d_W4\n",
    "    parameters['b4'] = b4 - alpha*d_b4\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Design of a Neural Network from scratch\n",
    "\n",
    "*************<IMP>*************\n",
    "Mention hyperparameters used and describe functionality in detail in this space\n",
    "- carries 1 mark\n",
    "\n",
    "Learning rate - rate at which model learns the training samples - alpha\n",
    "Epochs - Number of times the model goes through the training samples\n",
    "Learning rate decay - Rate at which learning rate reduces every epoch, time based decay\n",
    "Momentum - Used to speed up convergence, increases value in the direction of gradient\n",
    "        - helps prevent oscillations \n",
    "\n",
    "'''\n",
    "\n",
    "class NN:\n",
    "\n",
    "    ''' X and Y are dataframes '''\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        '''\n",
    "        Function that trains the neural network by taking x_train and y_train samples as input\n",
    "        '''\n",
    "        global parameters, X_test, Y_test\n",
    "        epochs = parameters['max_epochs']\n",
    "        decay_rate = parameters['decay']\n",
    "        #do_dropout = parameters['do_dropout']\n",
    "        x_train = []\n",
    "        \n",
    "        # create a list of training numpy samples\n",
    "        for index, row in X.iterrows():\n",
    "            x_train.append(np.array(row, np.longdouble).reshape(len(row),1))\n",
    "        #print(x_train)\n",
    "        y_train = Y.to_numpy()\n",
    "        y_test = Y_test.to_numpy()\n",
    "        \n",
    "        # store accuracy and loss which can later be used for observations\n",
    "        history = dict()\n",
    "        history['alpha'] = [parameters['alpha']]\n",
    "        history['train_acc'] = [0]\n",
    "        history['train_loss'] = [float('+inf')]\n",
    "        history['val_loss'] = [float('+inf')]\n",
    "        history['val_acc'] = [0]\n",
    "        \n",
    "        #batch_size = parameters['batch_size']\n",
    "        n_train = parameters['n_train'] \n",
    "        #batches = (n_train//batch_size) + 1\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            y_hats = []\n",
    "            y_test_hats = []  \n",
    "            \n",
    "            # batch size 1, default, batch gradient descent did not help us much\n",
    "            for i in range(len(x_train)):\n",
    "                y_pred,cache = forward_propagation(x_train[i], parameters)\n",
    "                y_hats.append(y_pred)\n",
    "                grads_cache = calculate_gradients(x_train[i], y_train[i], parameters, cache)\n",
    "                update_parameters(parameters, grads_cache)\n",
    "            \n",
    "            # compute loss for that epoch\n",
    "            train_loss = compute_loss(np.array(y_hats),y_train, parameters)\n",
    "            # compute accuracy for that epoch\n",
    "            train_acc = compute_acc(y_hats, y_train)\n",
    "            # check testing loss\n",
    "            y_test_hats = self.predict(X_test)\n",
    "            test_loss = compute_loss(np.array(y_test_hats),y_test, parameters)\n",
    "            # get testing accuracy\n",
    "            test_acc = compute_acc(y_test_hats, y_test)\n",
    "            # print and store all the values\n",
    "            print(f\"Epoch::{epoch+1}/{epochs} ======= loss: {train_loss} - accuracy: {train_acc}\\n\")\n",
    "            parameters['alpha'] = parameters['initial_alpha'] *(1/(1+decay_rate*(epoch+1)))\n",
    "            history['alpha'].append(parameters['alpha'])\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['val_loss'].append(test_loss)\n",
    "            history['val_acc'].append(test_acc)\n",
    "        print(\"Training finished\")\n",
    "        return history\n",
    "            \n",
    "    def predict(self,X):\n",
    "\n",
    "        \"\"\"\n",
    "        The predict function performs a simple feed forward of weights\n",
    "        and outputs yhat values \n",
    "\n",
    "        yhat is a list of the predicted value for df X\n",
    "        \"\"\"\n",
    "        # go through all training samples, convert to numpy, append prediction to a list and return\n",
    "        global parameters\n",
    "        y_hat = []\n",
    "        for index, row in X.iterrows():\n",
    "            x_test = np.array(row, np.longdouble).reshape(len(row),1)\n",
    "            y_pred, cache = forward_propagation(x_test, parameters,do_dropout = False)\n",
    "            y_hat.append(y_pred)\n",
    "        return y_hat\n",
    "\n",
    "    def CM(self, y_test ,y_test_obs):\n",
    "        '''\n",
    "        Prints confusion matrix \n",
    "        y_test is list of y values in the test dataset\n",
    "        y_test_obs is list of y values predicted by the model\n",
    "\n",
    "        '''\n",
    "        \n",
    "        for i in range(len(y_test)):\n",
    "            if(y_test_obs[i].flatten()[0]>0.6):\n",
    "                y_test_obs[i]=1\n",
    "            else:\n",
    "                y_test_obs[i]=0\n",
    "        print(\"Y_accc    \", y_test)\n",
    "        print(\"Y_test_obs\",y_test_obs)\n",
    "        cm=[[0,0],[0,0]]\n",
    "        fp=0\n",
    "        fn=0\n",
    "        tp=0\n",
    "        tn=0\n",
    "        for i in range(len(y_test)):\n",
    "            if(y_test[i]==1 and y_test_obs[i]==1):\n",
    "                tp=tp+1\n",
    "            if(y_test[i]==0 and y_test_obs[i]==0):\n",
    "                tn=tn+1\n",
    "            if(y_test[i]==1 and y_test_obs[i]==0):\n",
    "                fp=fp+1\n",
    "            if(y_test[i]==0 and y_test_obs[i]==1):\n",
    "                fn=fn+1\n",
    "        cm[0][0]=tn\n",
    "        cm[0][1]=fp\n",
    "        cm[1][0]=fn\n",
    "        cm[1][1]=tp\n",
    "\n",
    "        p= tp/(max(1,tp+fp))\n",
    "        r=tp/max(1,tp+fn)\n",
    "        f1=(2*p*r)/max(1,p+r)\n",
    "        \n",
    "        print(\"Confusion Matrix : \")\n",
    "        print(cm)\n",
    "        print(\"\\n\")\n",
    "        print(f\"Precision : {p}\")\n",
    "        print(f\"Recall : {r}\")\n",
    "        print(f\"F1 SCORE : {f1}\")\n",
    "        print(\"Accuracy : \", (tp+tn)/(tp+fp+tn+fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'F:\\\\Engi_Books\\\\Sem5\\\\MI\\\\UE18CS303_Assignment\\\\Assignment3\\\\data\\\\LBW_Dataset.csv' does not exist: b'F:\\\\Engi_Books\\\\Sem5\\\\MI\\\\UE18CS303_Assignment\\\\Assignment3\\\\data\\\\LBW_Dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0d888673f5f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mPATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\"F:\\Engi_Books\\Sem5\\MI\\UE18CS303_Assignment\\Assignment3\\data\\LBW_Dataset.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#df = cleaning(raw_df)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#df.describe()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Shubham\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Shubham\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Shubham\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Shubham\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Shubham\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'F:\\\\Engi_Books\\\\Sem5\\\\MI\\\\UE18CS303_Assignment\\\\Assignment3\\\\data\\\\LBW_Dataset.csv' does not exist: b'F:\\\\Engi_Books\\\\Sem5\\\\MI\\\\UE18CS303_Assignment\\\\Assignment3\\\\data\\\\LBW_Dataset.csv'"
     ]
    }
   ],
   "source": [
    "PATH = r\"F:\\Engi_Books\\Sem5\\MI\\UE18CS303_Assignment\\Assignment3\\data\\LBW_clean.csv\"\n",
    "df=pd.read_csv(PATH)\n",
    "#df = cleaning(raw_df)\n",
    "#df \n",
    "#df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df[\"Result\"]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df.drop([\"Result\"],axis = 1), Y, stratify = Y,\n",
    "                                                    test_size = 0.3, random_state = 1000000007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all the parameters, tweak here to see different results\n",
    "n_x = X_train.shape[1]\n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 256\n",
    "n_hidden3 = 512\n",
    "n_y = 1\n",
    "alpha =8e-6\n",
    "epochs = 100\n",
    "momentum = 0.999\n",
    "#patience = 50\n",
    "decay_rate = alpha/epochs\n",
    "parameters = initialize_parameters(n_x = n_x,\n",
    "                                   n_hidden1 = n_hidden1,\n",
    "                                   n_hidden3 = n_hidden3,\n",
    "                                    n_hidden2 = n_hidden2,\n",
    "                                   n_y = n_y, \n",
    "                                    alpha = alpha,\n",
    "                                   max_epochs = epochs,\n",
    "                                    n_train = n_train,\n",
    "                                   momentum = momentum,\n",
    "                                    n_test = n_test,\n",
    "                                   decay_rate=decay_rate)\n",
    "#print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = NN()\n",
    "history = neural_network.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = neural_network.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network.CM(Y_test.tolist(), y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history['alpha']) \n",
    "plt.ylabel('learning rate') \n",
    "plt.xlabel(\"Epochs:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history['train_acc'], label=\"Training acc\") \n",
    "plt.plot(history['val_acc'], label=\"Testing acc\") \n",
    "plt.legend()\n",
    "plt.ylabel('Accuracy') \n",
    "plt.xlabel(\"Epochs:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history['train_loss'], label=\"Training loss\") \n",
    "plt.plot(history['val_loss'], label=\"Testing loss\") \n",
    "plt.legend()\n",
    "plt.ylabel('Loss') \n",
    "plt.xlabel(\"Epochs:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
